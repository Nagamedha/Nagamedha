{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nagamedha/Nagamedha/blob/main/Level1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "523cc8c2-ab7f-4b52-86a9-2265ddd84485",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "523cc8c2-ab7f-4b52-86a9-2265ddd84485",
        "outputId": "5573b677-a0c3-4175-d75f-122a92eb181c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 18.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 489kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.51MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 2.96MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0/469, Loss: 2.3210\n",
            "Batch 50/469, Loss: 0.7457\n",
            "Batch 100/469, Loss: 0.3441\n",
            "Batch 150/469, Loss: 0.1659\n",
            "Batch 200/469, Loss: 0.1671\n",
            "Batch 250/469, Loss: 0.1057\n",
            "Batch 300/469, Loss: 0.1907\n",
            "Batch 350/469, Loss: 0.1370\n",
            "Batch 400/469, Loss: 0.1586\n",
            "Batch 450/469, Loss: 0.0933\n",
            "Train Loss: 0.3041, Train Accuracy: 90.63%\n",
            "Test Accuracy: 98.19%\n",
            "Saved best model with accuracy: 98.19%\n",
            "Epoch 2/20\n",
            "Batch 0/469, Loss: 0.0624\n",
            "Batch 50/469, Loss: 0.0658\n",
            "Batch 100/469, Loss: 0.0883\n",
            "Batch 150/469, Loss: 0.1416\n",
            "Batch 200/469, Loss: 0.0601\n",
            "Batch 250/469, Loss: 0.0431\n",
            "Batch 300/469, Loss: 0.1016\n",
            "Batch 350/469, Loss: 0.0441\n",
            "Batch 400/469, Loss: 0.0474\n",
            "Batch 450/469, Loss: 0.0597\n",
            "Train Loss: 0.1052, Train Accuracy: 96.93%\n",
            "Test Accuracy: 98.60%\n",
            "Saved best model with accuracy: 98.60%\n",
            "Epoch 3/20\n",
            "Batch 0/469, Loss: 0.0903\n",
            "Batch 50/469, Loss: 0.0526\n",
            "Batch 100/469, Loss: 0.1098\n",
            "Batch 150/469, Loss: 0.0761\n",
            "Batch 200/469, Loss: 0.1036\n",
            "Batch 250/469, Loss: 0.1795\n",
            "Batch 300/469, Loss: 0.0323\n",
            "Batch 350/469, Loss: 0.0672\n",
            "Batch 400/469, Loss: 0.0671\n",
            "Batch 450/469, Loss: 0.1345\n",
            "Train Loss: 0.0761, Train Accuracy: 97.75%\n",
            "Test Accuracy: 98.82%\n",
            "Saved best model with accuracy: 98.82%\n",
            "Epoch 4/20\n",
            "Batch 0/469, Loss: 0.0584\n",
            "Batch 50/469, Loss: 0.0725\n",
            "Batch 100/469, Loss: 0.0632\n",
            "Batch 150/469, Loss: 0.0723\n",
            "Batch 200/469, Loss: 0.1235\n",
            "Batch 250/469, Loss: 0.0756\n",
            "Batch 300/469, Loss: 0.1236\n",
            "Batch 350/469, Loss: 0.0634\n",
            "Batch 400/469, Loss: 0.0977\n",
            "Batch 450/469, Loss: 0.1009\n",
            "Train Loss: 0.0620, Train Accuracy: 98.24%\n",
            "Test Accuracy: 99.02%\n",
            "Saved best model with accuracy: 99.02%\n",
            "Epoch 5/20\n",
            "Batch 0/469, Loss: 0.0347\n",
            "Batch 50/469, Loss: 0.0657\n",
            "Batch 100/469, Loss: 0.0566\n",
            "Batch 150/469, Loss: 0.0495\n",
            "Batch 200/469, Loss: 0.0395\n",
            "Batch 250/469, Loss: 0.0161\n",
            "Batch 300/469, Loss: 0.1800\n",
            "Batch 350/469, Loss: 0.2198\n",
            "Batch 400/469, Loss: 0.0426\n",
            "Batch 450/469, Loss: 0.0940\n",
            "Train Loss: 0.0552, Train Accuracy: 98.32%\n",
            "Test Accuracy: 99.05%\n",
            "Saved best model with accuracy: 99.05%\n",
            "Epoch 6/20\n",
            "Batch 0/469, Loss: 0.0221\n",
            "Batch 50/469, Loss: 0.0243\n",
            "Batch 100/469, Loss: 0.0285\n",
            "Batch 150/469, Loss: 0.0463\n",
            "Batch 200/469, Loss: 0.0182\n",
            "Batch 250/469, Loss: 0.0112\n",
            "Batch 300/469, Loss: 0.0462\n",
            "Batch 350/469, Loss: 0.0103\n",
            "Batch 400/469, Loss: 0.1359\n",
            "Batch 450/469, Loss: 0.0203\n",
            "Train Loss: 0.0458, Train Accuracy: 98.52%\n",
            "Test Accuracy: 99.15%\n",
            "Saved best model with accuracy: 99.15%\n",
            "Epoch 7/20\n",
            "Batch 0/469, Loss: 0.0635\n",
            "Batch 50/469, Loss: 0.0421\n",
            "Batch 100/469, Loss: 0.0309\n",
            "Batch 150/469, Loss: 0.0224\n",
            "Batch 200/469, Loss: 0.0096\n",
            "Batch 250/469, Loss: 0.0285\n",
            "Batch 300/469, Loss: 0.0323\n",
            "Batch 350/469, Loss: 0.0939\n",
            "Batch 400/469, Loss: 0.0153\n",
            "Batch 450/469, Loss: 0.0868\n",
            "Train Loss: 0.0407, Train Accuracy: 98.75%\n",
            "Test Accuracy: 99.09%\n",
            "Epoch 8/20\n",
            "Batch 0/469, Loss: 0.0396\n",
            "Batch 50/469, Loss: 0.0363\n",
            "Batch 100/469, Loss: 0.0266\n",
            "Batch 150/469, Loss: 0.0872\n",
            "Batch 200/469, Loss: 0.0246\n",
            "Batch 250/469, Loss: 0.0107\n",
            "Batch 300/469, Loss: 0.0705\n",
            "Batch 350/469, Loss: 0.0353\n",
            "Batch 400/469, Loss: 0.0623\n",
            "Batch 450/469, Loss: 0.0345\n",
            "Train Loss: 0.0390, Train Accuracy: 98.79%\n",
            "Test Accuracy: 99.03%\n",
            "Epoch 9/20\n",
            "Batch 0/469, Loss: 0.0391\n",
            "Batch 50/469, Loss: 0.0108\n",
            "Batch 100/469, Loss: 0.0114\n",
            "Batch 150/469, Loss: 0.0111\n",
            "Batch 200/469, Loss: 0.0525\n",
            "Batch 250/469, Loss: 0.0409\n",
            "Batch 300/469, Loss: 0.0204\n",
            "Batch 350/469, Loss: 0.0303\n",
            "Batch 400/469, Loss: 0.0182\n",
            "Batch 450/469, Loss: 0.0685\n",
            "Train Loss: 0.0347, Train Accuracy: 98.92%\n",
            "Test Accuracy: 99.12%\n",
            "Epoch 10/20\n",
            "Batch 0/469, Loss: 0.0393\n",
            "Batch 50/469, Loss: 0.0207\n",
            "Batch 100/469, Loss: 0.0169\n",
            "Batch 150/469, Loss: 0.0490\n",
            "Batch 200/469, Loss: 0.0625\n",
            "Batch 250/469, Loss: 0.0180\n",
            "Batch 300/469, Loss: 0.0099\n",
            "Batch 350/469, Loss: 0.0113\n",
            "Batch 400/469, Loss: 0.0042\n",
            "Batch 450/469, Loss: 0.0285\n",
            "Train Loss: 0.0301, Train Accuracy: 99.05%\n",
            "Test Accuracy: 99.16%\n",
            "Saved best model with accuracy: 99.16%\n",
            "Epoch 11/20\n",
            "Batch 0/469, Loss: 0.0110\n",
            "Batch 50/469, Loss: 0.0081\n",
            "Batch 100/469, Loss: 0.0071\n",
            "Batch 150/469, Loss: 0.0374\n",
            "Batch 200/469, Loss: 0.0031\n",
            "Batch 250/469, Loss: 0.0096\n",
            "Batch 300/469, Loss: 0.0049\n",
            "Batch 350/469, Loss: 0.0227\n",
            "Batch 400/469, Loss: 0.0103\n",
            "Batch 450/469, Loss: 0.0069\n",
            "Train Loss: 0.0217, Train Accuracy: 99.31%\n",
            "Test Accuracy: 99.20%\n",
            "Saved best model with accuracy: 99.20%\n",
            "Epoch 12/20\n",
            "Batch 0/469, Loss: 0.0179\n",
            "Batch 50/469, Loss: 0.0015\n",
            "Batch 100/469, Loss: 0.0387\n",
            "Batch 150/469, Loss: 0.0100\n",
            "Batch 200/469, Loss: 0.0689\n",
            "Batch 250/469, Loss: 0.0012\n",
            "Batch 300/469, Loss: 0.0270\n",
            "Batch 350/469, Loss: 0.0078\n",
            "Batch 400/469, Loss: 0.0438\n",
            "Batch 450/469, Loss: 0.0100\n",
            "Train Loss: 0.0179, Train Accuracy: 99.42%\n",
            "Test Accuracy: 99.28%\n",
            "Saved best model with accuracy: 99.28%\n",
            "Epoch 13/20\n",
            "Batch 0/469, Loss: 0.0151\n",
            "Batch 50/469, Loss: 0.0319\n",
            "Batch 100/469, Loss: 0.0419\n",
            "Batch 150/469, Loss: 0.0048\n",
            "Batch 200/469, Loss: 0.0058\n",
            "Batch 250/469, Loss: 0.0043\n",
            "Batch 300/469, Loss: 0.0100\n",
            "Batch 350/469, Loss: 0.0405\n",
            "Batch 400/469, Loss: 0.0256\n",
            "Batch 450/469, Loss: 0.0569\n",
            "Train Loss: 0.0167, Train Accuracy: 99.44%\n",
            "Test Accuracy: 99.34%\n",
            "Saved best model with accuracy: 99.34%\n",
            "Epoch 14/20\n",
            "Batch 0/469, Loss: 0.0100\n",
            "Batch 50/469, Loss: 0.0029\n",
            "Batch 100/469, Loss: 0.0027\n",
            "Batch 150/469, Loss: 0.0045\n",
            "Batch 200/469, Loss: 0.0879\n",
            "Batch 250/469, Loss: 0.0077\n",
            "Batch 300/469, Loss: 0.0266\n",
            "Batch 350/469, Loss: 0.0045\n",
            "Batch 400/469, Loss: 0.0022\n",
            "Batch 450/469, Loss: 0.0102\n",
            "Train Loss: 0.0151, Train Accuracy: 99.50%\n",
            "Test Accuracy: 99.27%\n",
            "Epoch 15/20\n",
            "Batch 0/469, Loss: 0.0243\n",
            "Batch 50/469, Loss: 0.0031\n",
            "Batch 100/469, Loss: 0.0402\n",
            "Batch 150/469, Loss: 0.0163\n",
            "Batch 200/469, Loss: 0.0018\n",
            "Batch 250/469, Loss: 0.0265\n",
            "Batch 300/469, Loss: 0.0524\n",
            "Batch 350/469, Loss: 0.0764\n",
            "Batch 400/469, Loss: 0.0262\n",
            "Batch 450/469, Loss: 0.0024\n",
            "Train Loss: 0.0144, Train Accuracy: 99.53%\n",
            "Test Accuracy: 99.27%\n",
            "Epoch 16/20\n",
            "Batch 0/469, Loss: 0.0048\n",
            "Batch 50/469, Loss: 0.0077\n",
            "Batch 100/469, Loss: 0.0157\n",
            "Batch 150/469, Loss: 0.0008\n",
            "Batch 200/469, Loss: 0.0107\n",
            "Batch 250/469, Loss: 0.0060\n",
            "Batch 300/469, Loss: 0.0060\n",
            "Batch 350/469, Loss: 0.0018\n",
            "Batch 400/469, Loss: 0.0106\n",
            "Batch 450/469, Loss: 0.0054\n",
            "Train Loss: 0.0134, Train Accuracy: 99.55%\n",
            "Test Accuracy: 99.30%\n",
            "Epoch 17/20\n",
            "Batch 0/469, Loss: 0.0106\n",
            "Batch 50/469, Loss: 0.0031\n",
            "Batch 100/469, Loss: 0.0010\n",
            "Batch 150/469, Loss: 0.0015\n",
            "Batch 200/469, Loss: 0.0046\n",
            "Batch 250/469, Loss: 0.0065\n",
            "Batch 300/469, Loss: 0.0118\n",
            "Batch 350/469, Loss: 0.0082\n",
            "Batch 400/469, Loss: 0.0123\n",
            "Batch 450/469, Loss: 0.0024\n",
            "Train Loss: 0.0129, Train Accuracy: 99.56%\n",
            "Test Accuracy: 99.28%\n",
            "Epoch 18/20\n",
            "Batch 0/469, Loss: 0.0020\n",
            "Batch 50/469, Loss: 0.0036\n",
            "Batch 100/469, Loss: 0.0391\n",
            "Batch 150/469, Loss: 0.0314\n",
            "Batch 200/469, Loss: 0.0011\n",
            "Batch 250/469, Loss: 0.0161\n",
            "Batch 300/469, Loss: 0.0081\n",
            "Batch 350/469, Loss: 0.0189\n",
            "Batch 400/469, Loss: 0.0032\n",
            "Batch 450/469, Loss: 0.0131\n",
            "Train Loss: 0.0124, Train Accuracy: 99.59%\n",
            "Test Accuracy: 99.28%\n",
            "Epoch 19/20\n",
            "Batch 0/469, Loss: 0.0066\n",
            "Batch 50/469, Loss: 0.0133\n",
            "Batch 100/469, Loss: 0.0091\n",
            "Batch 150/469, Loss: 0.0098\n",
            "Batch 200/469, Loss: 0.0057\n",
            "Batch 250/469, Loss: 0.0550\n",
            "Batch 300/469, Loss: 0.0040\n",
            "Batch 350/469, Loss: 0.0085\n",
            "Batch 400/469, Loss: 0.0074\n",
            "Batch 450/469, Loss: 0.0151\n",
            "Train Loss: 0.0105, Train Accuracy: 99.67%\n",
            "Test Accuracy: 99.38%\n",
            "Saved best model with accuracy: 99.38%\n",
            "Epoch 20/20\n",
            "Batch 0/469, Loss: 0.0231\n",
            "Batch 50/469, Loss: 0.0038\n",
            "Batch 100/469, Loss: 0.0110\n",
            "Batch 150/469, Loss: 0.0022\n",
            "Batch 200/469, Loss: 0.0011\n",
            "Batch 250/469, Loss: 0.0114\n",
            "Batch 300/469, Loss: 0.0411\n",
            "Batch 350/469, Loss: 0.0024\n",
            "Batch 400/469, Loss: 0.0074\n",
            "Batch 450/469, Loss: 0.0103\n",
            "Train Loss: 0.0117, Train Accuracy: 99.63%\n",
            "Test Accuracy: 99.36%\n",
            "Training complete. Best Test Accuracy: 99.38%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6adb8ce51c47>:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(best_model_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 99.38%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import numpy as np\n",
        "\n",
        "# Generic data loader for any dataset\n",
        "def load_data(dataset_name=\"MNIST\", batch_size=128):\n",
        "    # Define default transformations\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))  # Default normalization\n",
        "    ])\n",
        "\n",
        "    # Load dataset\n",
        "    if dataset_name.upper() == \"MNIST\":\n",
        "        train_dataset = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "        test_dataset = torchvision.datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "        input_channels = 1\n",
        "        input_size = 28\n",
        "        num_classes = 10\n",
        "\n",
        "    elif dataset_name.upper() == \"CIFAR10\":\n",
        "        transform = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "        train_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "        test_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "        input_channels = 3\n",
        "        input_size = 32\n",
        "        num_classes = 10\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Dataset '{dataset_name}' not supported.\")\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    return train_loader, test_loader, input_channels, input_size, num_classes\n",
        "\n",
        "# Flexible CNN Model\n",
        "class FlexibleCNN(nn.Module):\n",
        "    def __init__(self, input_channels, input_size, num_classes):\n",
        "        super(FlexibleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * (input_size // 4) * (input_size // 4), 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = self.dropout(self.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, optimizer, loss_fn, device):\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Metrics\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            print(f\"Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Train Loss: {running_loss / len(train_loader):.4f}, Train Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy\n",
        "\n",
        "# Main function\n",
        "def main(dataset_name=\"MNIST\", epochs=20, batch_size=128):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load dataset\n",
        "    train_loader, test_loader, input_channels, input_size, num_classes = load_data(dataset_name, batch_size)\n",
        "\n",
        "    # Initialize model, optimizer, and scheduler\n",
        "    model = FlexibleCNN(input_channels, input_size, num_classes).to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "    scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training and evaluation\n",
        "    best_accuracy = 0.0\n",
        "    best_model_path = f\"best_{dataset_name.lower()}_model.pth\"\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "        train_model(model, train_loader, optimizer, loss_fn, device)\n",
        "        accuracy = evaluate_model(model, test_loader, device)\n",
        "\n",
        "        # Save the best model\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"Saved best model with accuracy: {best_accuracy:.2f}%\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    print(f\"Training complete. Best Test Accuracy: {best_accuracy:.2f}%\")\n",
        "\n",
        "    # Load and test the best model\n",
        "    model.load_state_dict(torch.load(best_model_path))\n",
        "    evaluate_model(model, test_loader, device)\n",
        "\n",
        "# Run the program\n",
        "if __name__ == \"__main__\":\n",
        "    main(dataset_name=\"MNIST\", epochs=20, batch_size=128)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d973384-fc00-42bb-a835-09b07e5a8903",
      "metadata": {
        "id": "7d973384-fc00-42bb-a835-09b07e5a8903"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}